[
  {
    "id": "263e9933-5fca-80a2-a855-dc4f4e07d9d1",
    "created_time": "2025-09-03T19:20:00.000Z",
    "title": "How is code sandboxing on platforms like LeetCode designed?",
    "date": "2025-09-03",
    "description": [
      {
        "component": "paragraph",
        "text": "It is a very sensitive part of the design for coding platforms like LeetCode and GeeksforGeeks. Allowing users to send in code and run it on your server is a dangerous choice. On the surface, it just looks like running Docker containers for execution, but that too needs a lot of modifications since it shares the kernel with the host machine."
      },
      {
        "component": "paragraph",
        "text": "Option 1: Docker containers"
      },
      {
        "component": "bulleted_list_item",
        "text": "Prebuilt language-specific containers are used to execute code in a sandbox."
      },
      {
        "component": "bulleted_list_item",
        "text": "They use namespaces like the process namespace that isolates the container processes, and the network namespace which disables any network access to the host."
      },
      {
        "component": "bulleted_list_item",
        "text": "cgroups are used to put resource consumption limits on the processes of these containers. For example, for every test case, max use 2s or 256MB RAM."
      },
      {
        "component": "bulleted_list_item",
        "text": "seccomp is added to protect against any system call generated by the user code. Filesystem access is controlled for the code execution, and code can only access the memory it needs for fair use."
      },
      {
        "component": "bulleted_list_item",
        "text": "Though a lot of security modifications are needed, it has benefits, to begin with, it is faster to start up and has low memory overhead."
      },
      {
        "component": "bulleted_list_item",
        "text": "The downside is the kernel is shared; if a kernel escape exploit is found, the host machine can be compromised."
      },
      {
        "component": "bulleted_list_item",
        "text": "Preferable for low-risk loads. Suitable for interpreted languages where dangerous syscalls are either not present (like no mount() syscall in Python) or are made via the interpreter."
      },
      {
        "component": "paragraph",
        "text": "Option 2: Firecracker MicroVMs"
      },
      {
        "component": "bulleted_list_item",
        "text": "Lightweight virtual machines, open-sourced and heavily used and developed by AWS."
      },
      {
        "component": "bulleted_list_item",
        "text": "Startups are fast but slower compared to Docker containers."
      },
      {
        "component": "bulleted_list_item",
        "text": "Stronger isolation though, as the kernel is its own, not shared. Any syscall stays in the VM and does not reach the host machine."
      },
      {
        "component": "bulleted_list_item",
        "text": "Strongly bounded memory, as a complete OS is running and cannot read memory outside of it."
      },
      {
        "component": "bulleted_list_item",
        "text": "Usable for high-risk loads like compiled languages such as C++ and Rust, where direct syscalls can be made by the compiled machine code."
      },
      {
        "component": "paragraph",
        "text": "Platforms like LeetCode use a hybrid model for code sandboxing. For trusted clients and loads, they use Docker-based containers, while for risky and untrusted submissions, microVMs are used."
      },
      {
        "component": "paragraph",
        "text": "Question‚Ä¶AWS Fargate uses Firecracker underneath and Google Cloud Run uses gVisor (another virtualization tech with user-space kernels, giving more isolation and performance in between Firecracker and Docker containers). They are good with isolation, so why do these platforms not use services like these?"
      },
      {
        "component": "paragraph",
        "text": "These serverless products are designed for long-running tasks like APIs and workers. While for online judge architecture you have short-lived and bulk tasks. Code is executed in seconds and then it is over. If one uses these products, they would need to spin up a container for every submission, which, not to be ignored, is far slower than a native Docker container spin-up or Firecracker startup. Another problem is that very fine-grained control is required on resource consumption with user-submitted programs, i.e., max 2s per test case or 256MB max RAM usage. While in Fargate or Cloud Run you get to manage the container size and memory limit, you can‚Äôt set up cgroup limits like we discussed in the earlier part. And serverless is 10x more costly than running your own orchestration of containers and microVMs."
      }
    ]
  },
  {
    "id": "262e9933-5fca-80e5-8772-c7a7e5c70491",
    "created_time": "2025-09-02T19:13:00.000Z",
    "title": "Tech behind torrents",
    "date": "2025-09-02",
    "description": [
      {
        "component": "paragraph",
        "text": "I‚Äôve always been fascinated by the tech behind torrents, no central server hosting files, and yet files can download at lightning speeds. There‚Äôs no single server keeping track of which peer has which piece, but somehow the whole file gets downloaded. How does it all work?"
      },
      {
        "component": "paragraph",
        "text": "Torrents operate on a P2P (peer-to-peer) file sharing network. Instead of transferring one giant file, the content is broken down into many smaller pieces."
      },
      {
        "component": "paragraph",
        "text": "The .torrent file contains hashes for these pieces, which serve two purposes:"
      },
      {
        "component": "bulleted_list_item",
        "text": "They tell the client which pieces to download."
      },
      {
        "component": "bulleted_list_item",
        "text": "They verify file integrity, ensuring that downloaded pieces haven‚Äôt been tampered with."
      },
      {
        "component": "paragraph",
        "text": "In a torrent network, there are two main types of users:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Leechers: users who are downloading file pieces."
      },
      {
        "component": "bulleted_list_item",
        "text": "Seeders: users who are uploading file pieces."
      },
      {
        "component": "paragraph",
        "text": "A single user can be both at once, as soon as you download a piece, you can immediately begin uploading it to others."
      },
      {
        "component": "paragraph",
        "text": "To download pieces, your client needs to know where peers are. In the early days, torrents relied heavily on a central tracker server that acted like a directory, keeping a list of which users had which files."
      },
      {
        "component": "paragraph",
        "text": "Modern torrent clients, however, are more decentralized. While trackers still exist, they‚Äôre no longer the only way to find peers. Instead, clients use a Distributed Hash Table (DHT):"
      },
      {
        "component": "paragraph",
        "text": "Once you open your client it goes to the tracker to get the addresses of some nearby peers. Every peer stores a local Distributed hash table, which contains address:port ‚Üí info hashes of files that can be downloaded from that peer. From one peer to another the search goes on until you get the nearest peer that has the file you are looking for. And with that one downloads the whole file without relying on a single server."
      }
    ]
  },
  {
    "id": "261e9933-5fca-8021-9db8-c34dc95f1473",
    "created_time": "2025-09-01T19:18:00.000Z",
    "title": "What are SOAP and GraphQL",
    "date": "2025-09-01",
    "description": [
      {
        "component": "paragraph",
        "text": "REST is so commonplace that we never dig deeper to learn that there other ways of designing APIs."
      },
      {
        "component": "paragraph",
        "text": "üîπ GraphQL"
      },
      {
        "component": "bulleted_list_item",
        "text": "GraphQL is a query language for APIs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Instead of the server deciding what data you get (like in REST) the client asks exactly what it needs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Instead of multiple endpoints, there is only one flexible endpoint."
      },
      {
        "component": "bulleted_list_item",
        "text": "You just define a schema and data resolvers(methods to fetch the data), the client sends in dynamic requests around the schema with what it needs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Like you can get {‚Äúquery‚Äù : \"{ user(id: 123) { name email orders { id total } } }‚Äù} no specific handler needs to be coded on the backend for different queries."
      },
      {
        "component": "bulleted_list_item",
        "text": "This reduces over-fetching/under-fetching problems common with REST. Just return exactly what was asked."
      },
      {
        "component": "paragraph",
        "text": "üîπ SOAP (Simple Object Access Protocol)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Heavily used in enterprise and legacy systems (e.g., banking, telecom, government)."
      },
      {
        "component": "bulleted_list_item",
        "text": "Operates over multiple protocols (HTTP, SMTP, etc.), not just HTTP."
      },
      {
        "component": "bulleted_list_item",
        "text": "Uses XML as request/response objects. Has a strict schema defined for the objects. If req/res does not follow the schema, the API call fails. "
      },
      {
        "component": "bulleted_list_item",
        "text": "Comes with strict standards and built-in security (WS-Security), Portions like certain fields of the req/res object can be encrypted separately in the application layer itself! while in REST it is done in the Transport layer."
      },
      {
        "component": "bulleted_list_item",
        "text": "Many banking systems use SOAP because it provides stricter control over payloads and is generally considered more secure."
      }
    ]
  },
  {
    "id": "260e9933-5fca-8052-8b02-d0f38eb99378",
    "created_time": "2025-08-31T18:14:00.000Z",
    "title": "What are preflight requests and why are they important?",
    "date": "2025-08-31",
    "description": [
      {
        "component": "paragraph",
        "text": "No, they are not redundant. Remember last time you set Access-Control-Allow-Origin = \"*\"  in your backend? Well, that can be a serious problem!"
      },
      {
        "component": "paragraph",
        "text": "A preflight request is a type of request that web browsers send before making a cross-origin HTTP request. This is part of the CORS (Cross-Origin Resource Sharing) mechanism."
      },
      {
        "component": "paragraph",
        "text": "Browsers send preflight requests before any request that is considered non-simple. Non-simple requests include:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Methods like PUT, DELETE, or PATCH."
      },
      {
        "component": "bulleted_list_item",
        "text": "POST requests with content types other than application/x-www-form-urlencoded, multipart/form-data, or text/plain."
      },
      {
        "component": "bulleted_list_item",
        "text": "Requests with custom headers (e.g., Authorization, X-API-KEY, etc.)."
      },
      {
        "component": "paragraph",
        "text": "Preflight requests use the OPTIONS HTTP method to check with the backend server whether the actual request is allowed from the requesting origin. Only if the backend responds with the proper CORS headers (essentially saying the origin is allowed) will the browser proceed with the real request."
      },
      {
        "component": "paragraph",
        "text": "This is especially important when you use cookies for authentication in your backend. When a website makes a request to a server, the browser automatically includes the cookies associated with that server‚Äôs origin. Even though frontend JavaScript cannot access cookies from other origins or tabs, the browser attaches them to outgoing requests for that origin, for authentication purpose of that request."
      },
      {
        "component": "paragraph",
        "text": "If preflight checks didn‚Äôt exist, or if the backend allowed all origins (*), then any malicious website could send requests to your backend from the victim‚Äôs browser. Since the browser would automatically attach authentication cookies, the attacker could perform actions on behalf of the logged-in user on you backend. This kind of exploit is called CSRF (Cross-Site Request Forgery)."
      },
      {
        "component": "paragraph",
        "text": "Preflight requests help prevent this by ensuring that only trusted origins are allowed to perform non-simple requests. They cannot be bypassed in a normal browser environment. Preflight requests are important aren‚Äôt they?"
      }
    ]
  },
  {
    "id": "25fe9933-5fca-809a-bba4-e928fc2db523",
    "created_time": "2025-08-30T19:43:00.000Z",
    "title": "How is a rate limiter designed for distributed environments?",
    "date": "2025-08-30",
    "description": [
      {
        "component": "paragraph",
        "text": "Probably, this will be something more than what you‚Äôve previously read about the topic."
      },
      {
        "component": "paragraph",
        "text": "Most commonly, a rate limiter is implemented in an API Gateway. An API Gateway is essentially a reverse proxy with fully managed services like rate limiting, SSL termination, authentication, IP whitelisting, and static content handling. It can convert request/response object formats (e.g., XML ‚Üí JSON) or perform authentication for user requests without ever hitting your backend service. Some commonly used API gateways are AWS API Gateway, Kong Gateway and Apigee. Rate limiting can also be implemented with the backend services without an API Gateway, depending on product requirements."
      },
      {
        "component": "paragraph",
        "text": "There are multiple algorithms used for rate limiting, but the most common are Token Bucket and Leaky Bucket. The idea is pretty straightforward."
      },
      {
        "component": "paragraph",
        "text": "In the Token Bucket algorithm, each user has a bucket of tokens that is refilled at a constant rate. When a user request arrives, if a token is present in the bucket, the request proceeds; otherwise, it is dropped. This is the easy part. The harder part is the refilling process. For millions of users, there would need to be millions of refilling processes accessing a shared Redis backend, which would result in a nightmare of Redis requests."
      },
      {
        "component": "paragraph",
        "text": "A solution to this is to remove the explicit refiller. Instead, you store some additional information the last access timestamp. For example, you store:"
      },
      {
        "component": "paragraph",
        "text": "user123 : { tokens_left, last_access_timestamp }"
      },
      {
        "component": "paragraph",
        "text": "When a new request comes in, you calculate how many tokens would have been refilled since the last access and add that to tokens_left. Based on this, you can decide whether the request is allowed."
      },
      {
        "component": "paragraph",
        "text": "However, this approach has a caveat in distributed systems. It‚Äôs no longer just a simple counter increment, it requires calculation, which means a two-way round trip to the server. First, the server reads the last timestamp from Redis, then calculates the updated token count, and finally writes it back. This can create race conditions."
      },
      {
        "component": "paragraph",
        "text": "For example, suppose two push notifications need to be sent to user123 at exactly the same time, and they are handled by two different nodes. Node A fetches the last timestamp, calculates the updated value, and prepares to update Redis. Meanwhile, Node B fetches the now stale timestamp and performs its own calculation. This causes inconsistencies and may occasionally allow requests to go over the limit."
      },
      {
        "component": "paragraph",
        "text": "The final solution to this problem is to store only the access timestamps in a sorted set in Redis. The set stores only a range of timestamps ‚Äî for example, at most the last 3 timestamps within the last 10 seconds. The size of the set then acts as the counter. A new timestamp is added only if the set size is below the limit; otherwise, the request is dropped. This can be done in one atomic transaction using built-in Redis functions such as:"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZREMRANGEBYSCORE (remove elements outside the time range)"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZADD (add the new timestamp)"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZCARD (count the size)"
      },
      {
        "component": "paragraph",
        "text": "A question‚Ä¶.if Redis can do all this, why can‚Äôt it also calculate the token refill (tokens_left + (current_timestamp - last_timestamp)/rate) in a single transaction? The reason is that Redis has no built-in function for this calculation, so backend logic is still needed. However, this can be solved by using Redis Lua scripts, which allows you to define custom logic and execute it atomically. That way, the Token Bucket approach can also work in one single atomic transaction!"
      },
      {
        "component": "paragraph",
        "text": "Cool reference article for this : https://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/"
      }
    ]
  },
  {
    "id": "25ee9933-5fca-80c1-b06b-d613f8fb768f",
    "created_time": "2025-08-29T17:40:00.000Z",
    "title": "What happens in the backend when you do a UPI transaction?",
    "date": "2025-08-29",
    "description": [
      {
        "component": "paragraph",
        "text": "When you do a UPI transaction, the payment app extracts the Virtual Payment Address (VPA or UPI ID) from the QR code or the contact number. Payment apps like Paytm, GPay, and PhonePe are called Payment Service Provider (PSP) apps. The transaction request is sent to the NPCI (National Payments Corporation of India), which maintains a directory that maps VPAs to bank accounts."
      },
      {
        "component": "paragraph",
        "text": "The request is routed to the issuer bank (payer‚Äôs bank). It validates the request using the UPI PIN and the payer‚Äôs account balance."
      },
      {
        "component": "paragraph",
        "text": "The bank then sends a debit confirmation to the NPCI UPI switch, which routes a credit request to the receiver‚Äôs bank."
      },
      {
        "component": "paragraph",
        "text": "The settlement in the banks reflects instantly for the user, but the actual money transfer between banks happens through the RBI‚Äôs payment system. After regular intervals, a net transaction amount is calculated and fund transfers are settled between banks."
      },
      {
        "component": "paragraph",
        "text": "One important aspect of this whole setup is security. When you enter the UPI PIN, you do it through a secure keypad designed with additional safeguards. The keypad is a very peculiar case it is not a component of the payment app instead, it is provided by the NPCI‚Äôs UPI SDK."
      },
      {
        "component": "paragraph",
        "text": "This is an isolated UI component‚Äîno input is registered through the standard input channels of Android or iOS that are used by regular keyboard apps. As soon as a keystroke is registered, the digit is immediately encrypted by the SDK. The UPI PIN never exists in memory as plain text. Every input block is encrypted using the public key provided by the payer‚Äôs bank. The corresponding private key is securely stored only with the bank. Not even NPCI has access to your PIN."
      },
      {
        "component": "paragraph",
        "text": "Even banks do not store the PIN in plain text. They use symmetric encryption to encrypt the PIN and store it inside a Hardware Security Module (HSM), a tamper-proof device that self-erases data if tampering is attempted. The PIN entered by the user is decrypted using the private key, re-encrypted inside the HSM using the symmetric key, and then compared against the stored encrypted block."
      },
      {
        "component": "paragraph",
        "text": "That‚Äôs a lot going for a transaction that happens almost instantly!"
      }
    ]
  },
  {
    "id": "25de9933-5fca-8075-a89a-c15181577fe4",
    "created_time": "2025-08-28T16:01:00.000Z",
    "title": "Proxy, Forward Proxy and Reverse Proxy what are they?",
    "date": "2025-08-28",
    "description": [
      {
        "component": "paragraph",
        "text": "A Proxy is basically a layer that sits between the server and the client. Whenever a request‚Äìresponse cycle takes place in your application, it goes through the proxy. One might find a lot of use cases as to why a proxy might be required, and depending on the use case, proxies can be broadly divided into two categories: Forward Proxy and Reverse Proxy."
      },
      {
        "component": "paragraph",
        "text": "üîπ Forward Proxy"
      },
      {
        "component": "paragraph",
        "text": "A forward proxy sits in front of the client. When the client sends a request, it first passes through the forward proxy before reaching the server. The server sees the proxy as the requester, not the actual client."
      },
      {
        "component": "paragraph",
        "text": "Common use cases:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Enforcing corporate/institutional policies (e.g., blocking certain websites)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Anonymizing client identity, So all the server sees is the ip address of the proxy server requesting the resource"
      },
      {
        "component": "bulleted_list_item",
        "text": "Forward proxy acts as an abstraction layer for the clients."
      },
      {
        "component": "paragraph",
        "text": "üîπ Reverse Proxy"
      },
      {
        "component": "paragraph",
        "text": "A reverse proxy, on the other hand, sits in front of the server. The client makes a request, but it only communicates with the reverse proxy, which then forwards the request to the appropriate server on behalf of the client. Here, the client never directly interacts with the actual server."
      },
      {
        "component": "paragraph",
        "text": "Common use cases:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Load balancing across multiple servers."
      },
      {
        "component": "bulleted_list_item",
        "text": "Routing requests to different services, i.e. yourdomain.com/service1 and service2 maybe running on the same server but on different ports, proxy server can direct the traffic accordingly."
      },
      {
        "component": "bulleted_list_item",
        "text": "Enhancing security by hiding server details(Abstraction layer for the server)"
      },
      {
        "component": "paragraph",
        "text": "In short:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Forward Proxy ‚Üí client-facing (protects/manages the client)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Reverse Proxy ‚Üí server-facing (protects/manages the server)"
      }
    ]
  },
  {
    "id": "25ce9933-5fca-805b-8041-df7e15ca1191",
    "created_time": "2025-08-27T17:38:00.000Z",
    "title": "How do food delivery apps like swiggy, zomato find the nearest restaurants in milliseconds",
    "date": "2025-08-27",
    "description": [
      {
        "component": "paragraph",
        "text": "When you start thinking about it, the problem looks very simple: just store the restaurants‚Äô coordinates, and once the user opens the app, calculate the distance to the restaurants, sort them, and return the results. Easy, right? Yes, until your app grows beyond being a pet project. When the user base reaches millions or even billions, optimized approaches are required."
      },
      {
        "component": "paragraph",
        "text": "One way to handle this is by using geo-enabled databases and spatial indexing."
      },
      {
        "component": "paragraph",
        "text": "For example, insert your restaurants into a geo-enabled database like PostGIS (Postgres) or MongoDB. Then build spatial indexes on the database. Internally, these use R-Trees (rectangle trees), where nearby restaurants are stored within the same rectangles."
      },
      {
        "component": "paragraph",
        "text": "When a user opens the app at a given location (lat, lon), the backend runs a query with a radius (e.g., all restaurants within 5 km). The index filters out relevant restaurants using bounding rectangles with the user location rectangle, and then the exact distance is calculated for the candidates. This is a workable approach when the number of rows is in the millions."
      },
      {
        "component": "paragraph",
        "text": "However, when dealing with billions of rows, geohashing is often used."
      },
      {
        "component": "paragraph",
        "text": "In this setup, the world is divided into grid cells, and each cell is assigned a unique ID (a geohash). Locations that are close to each other have similar geohash prefixes. The algorithm for computing a geohash is straightforward:"
      },
      {
        "component": "numbered_list_item",
        "text": "Take the user‚Äôs longitude and latitude. The longitude range is [-180, 180] and the latitude range is [-90, 90]."
      },
      {
        "component": "numbered_list_item",
        "text": "Create a bit array (size depends on the precision required). Each geohash character requires 5 bits."
      },
      {
        "component": "numbered_list_item",
        "text": "Start with the longitude: if it lies in the upper half of the range, set the bit; otherwise, leave it unset."
      },
      {
        "component": "numbered_list_item",
        "text": "Halve the range (e.g., [0, 180]) and do the same for latitude to set the next bit."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat this process until the array is filled."
      },
      {
        "component": "numbered_list_item",
        "text": "Take 5 bits at a time, convert them to base32, and map them to a character from '0123456789bcdefghjkmnpqrstuvwxyz'."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat until you obtain the final geohash string."
      },
      {
        "component": "paragraph",
        "text": "The more characters in the geohash, the more precise the location. For example:"
      },
      {
        "component": "bulleted_list_item",
        "text": "5 characters ‚Üí ~4.9 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "6 characters ‚Üí ~1.2 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "7 characters ‚Üí ~152 m precision"
      },
      {
        "component": "paragraph",
        "text": "Food delivery apps usually use 6‚Äì7 characters for querying. The query simply looks for restaurants with a geohash equal to the user‚Äôs geohash or its neighbors. This is a significantly faster approach than calculating distances for every restaurant in the database. üòÇ"
      }
    ]
  },
  {
    "id": "25be9933-5fca-8026-8dbd-c96403b39a0c",
    "created_time": "2025-08-26T16:34:00.000Z",
    "title": "Elasticsearch: What is it really, and why is it so fast?",
    "date": "2025-08-26",
    "description": [
      {
        "component": "paragraph",
        "text": "Elasticsearch is a search engine program that sits on top of your database to make your search queries blazingly fast. Where SQL might take minutes Elasticsearch responds within milliseconds."
      },
      {
        "component": "paragraph",
        "text": "It supports full text searches, aggregations, geo queries, autocompletes and a lot more. "
      },
      {
        "component": "paragraph",
        "text": "The architecture involved in setting up Elasticsearch system is as follows : "
      },
      {
        "component": "bulleted_list_item",
        "text": "Data source - The source of Truth : Postgres, MySQL, MongoDB"
      },
      {
        "component": "bulleted_list_item",
        "text": "Ingestion pipeline- To  copy or stream the data into Elasticsearch from the database."
      },
      {
        "component": "bulleted_list_item",
        "text": "Elasticsearch cluster - Multiple nodes with primary and replica shards of indexes(A big Index over the whole database is broken down into multiple smaller shards so that you can query all of the index in one go with parallel searches on these shards pretty clever!)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Search API - search queries are done via REST API (JSON over HTTP)"
      },
      {
        "component": "paragraph",
        "text": "Okay Elasticsearch is great and all but how does it do it?"
      },
      {
        "component": "paragraph",
        "text": "A very rudimentary view of the inner workings can be taken by the following illustration :"
      },
      {
        "component": "paragraph",
        "text": "1Ô∏è‚É£ Data ingestion in the Elasticsearch - data is inserted using POST API"
      },
      {
        "component": "paragraph",
        "text": "POST /book/_doc/1 "
      },
      {
        "component": "paragraph",
        "text": "{‚Äútitle‚Äù : ‚ÄúHarry  Potter‚Äù, ‚Äúauthor‚Äù : ‚ÄúJ. K. Rowling‚Äù, ‚Äúyear‚Äù : 1997}"
      },
      {
        "component": "paragraph",
        "text": "2Ô∏è‚É£ Inverted indexes are created - Think of these indexes as the word indexes found at the end of books. Tokens are mapped to the documents they occurred in."
      },
      {
        "component": "paragraph",
        "text": "‚ÄúHarry‚Äù ‚Üí Doc1"
      },
      {
        "component": "paragraph",
        "text": "‚ÄúPotter‚Äù ‚Üí Doc1"
      },
      {
        "component": "paragraph",
        "text": "‚ÄúHogwarts‚Äù ‚Üí Doc2"
      },
      {
        "component": "paragraph",
        "text": "3Ô∏è‚É£ Scoring and Ranking for search - Search inputs are broken into tokens and matched with the relevant documents using the Inverted Indexes. Docs having the highest occurrence of the token are given higher scores. Docs which contain the rarer word are given higher score(‚ÄùHogwarts‚Äù is rarer than ‚Äúthe‚Äù). The higher the score the higher the rank."
      },
      {
        "component": "paragraph",
        "text": "4Ô∏è‚É£ Indexes are broken into smaller shards and spread across multiple nodes, for faster lookups."
      },
      {
        "component": "paragraph",
        "text": "One more notable property of Elasticsearch is that it stores the fields in a columnar fashion. Which significantly boosts the aggregation speeds. Now to get averages instead of going to multiple rows you are only required to traverse all the entries in the column."
      },
      {
        "component": "paragraph",
        "text": "What it looks like is : "
      },
      {
        "component": "paragraph",
        "text": "Field : year ‚Üí [1997 2005 1996]"
      },
      {
        "component": "paragraph",
        "text": "Field : prices ‚Üí [20 15 25] "
      },
      {
        "component": "paragraph",
        "text": "That sums up, absolute basics one should know about Elasticsearch."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-80d3-9531-cab57ede7228",
    "created_time": "2025-08-25T14:42:00.000Z",
    "title": "Bloom Filters? You've probably read the name somewhere.",
    "date": "2025-08-25",
    "description": [
      {
        "component": "paragraph",
        "text": "Bloom filter is a data structure used to test whether an element is a member of a set or not. It is probabilistic in nature, what that means is that sometimes it may return false positives (i.e., it may return true even if the element does not exist in the set), while it never returns a false negative. If the bloom filter says the element is not a member then it is not a member 100%. It trades a little bit of accuracy for big gains in speed and memory efficiency."
      },
      {
        "component": "paragraph",
        "text": "Bloom filters are very handy in large scale systems like in key value stores, distributed HashMap where filtering out which node might contain the element saves a lot of time."
      },
      {
        "component": "paragraph",
        "text": "The implementation for bloom filters is very easy to be honest, all you need is a bit array and some hash functions. Let‚Äôs take an example :"
      },
      {
        "component": "paragraph",
        "text": "Consider a bit array of size 10 = [0 0 0 0 0 0 0 0 0 0] and 2 hash functions H1 and H2"
      },
      {
        "component": "paragraph",
        "text": "Let‚Äôs insert a key ‚Äúcat‚Äù in our filter :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(‚Äùcat‚Äù) = 2; idx2 =  H2(‚Äùcat‚Äù) = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "Set all the bits at indexes idx1 and idx2 in the bit array."
      },
      {
        "component": "bulleted_list_item",
        "text": "Updated bit array : [0 0 1 0 0 0 0 1 0 0]"
      },
      {
        "component": "paragraph",
        "text": "We‚Äôve inserted the key."
      },
      {
        "component": "paragraph",
        "text": "Now to query a key ‚Äúdog‚Äù follow these steps :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(‚Äùdog‚Äù) = 4; idx2 = H2(‚Äùdog‚Äù)  = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "check all the bits in the bit array at the idx1 and idx2."
      },
      {
        "component": "bulleted_list_item",
        "text": "If all the bits are set the element is in the set else not. Clearly dog is not a member."
      },
      {
        "component": "paragraph",
        "text": "This procedure of querying explains the reason for false positives as there might be collisions, the bits might be set by some other key but match the same pattern of another key not in the set."
      },
      {
        "component": "paragraph",
        "text": "In production the size of the bit arrays is reasonable, for storing 1 Million items the size required is 9.2 Million bits roughly 1.2 MB and 7 hash functions. The setup responds with 1% of false positives."
      },
      {
        "component": "paragraph",
        "text": "When not to use a Bloom filter (important to know)"
      },
      {
        "component": "paragraph",
        "text": "1Ô∏è‚É£ When false positives are not allowed. Like Checking if a password hash exists in a breach database. A false positive could wrongly tell a user their password is compromised."
      },
      {
        "component": "paragraph",
        "text": "2Ô∏è‚É£ When the dataset keeps on changing frequently, because bloom filters do not provide a method for key deletion as two or more keys might have set a common bit. But aren‚Äôt key value stores meant to be constantly changing datasets? Well yes they are! but they have a very clever solution in place which can be discussed in some other article."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-8076-97c5-c03d05e7e1fb",
    "created_time": "2025-08-25T10:13:00.000Z",
    "title": "You heard MongoDB follows ACID properties while other NoSQL DBs don‚Äôt. Do you know How and Why?",
    "date": "2025-08-24",
    "description": [
      {
        "component": "paragraph",
        "text": "TL;DR : NoSQL databases are BASE(Basically Available Soft State Eventually Consistent) by choice not by any limitation to NoSQL data storage format. ACID databases have a very specific use case in scenarios where consistency can‚Äôt be compromised say for Banking applications, Healthcare data storage. MongoDB tries to enter these use cases by being ACID. "
      },
      {
        "component": "paragraph",
        "text": "Long Answer : Unlike SQL databases, which were designed for single-node architectures, NoSQL databases must prioritize availability across multiple nodes. At scale, partition tolerance is a necessity if there‚Äôs a network outage between nodes, the database should still function. To achieve this, many NoSQL systems choose to be AP (Available and Partition-tolerant) as per the CAP theorem, since they cannot be fully consistent and AP at the same time."
      },
      {
        "component": "paragraph",
        "text": "But this still doesn‚Äôt explain why most NoSQL databases aren‚Äôt ACID. The term ACID, made popular by SQL databases, requires that a compliant database ensures:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity: In a transaction either all the operations are successful or none of them are."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency: After a transaction the DB should remain in a valid state, always respecting constraints such as schemas."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation: Every transaction should happen in Isolation, such that no other transaction is affected. This is often implemented using MVCC (Multi-Version Concurrency Control), which I‚Äôll cover later."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability: Once committed, a transaction is permanent and crash-safe, often implemented via Write-Ahead Logging (WAL), also discussed later."
      },
      {
        "component": "paragraph",
        "text": "By definition ACID appears a transaction specific property, no relationship with what it has to do with the database being SQL or NoSQL."
      },
      {
        "component": "paragraph",
        "text": "First thing an ACID database should have atomicity and durability which is often implemented using Write ahead logging(WAL) = a special sequential log file where all the changes are written before they are applied to the actual database. Secondly for Isolation, typically achieved through Multi version concurrency Control(MVCC) Instead of overwriting data mid-transaction, the database maintains multiple versions: one before the transaction and one after it completes. This ensures other operations aren‚Äôt affected by an incomplete transaction. A less common alternative is to enforce isolation using locking mechanisms."
      },
      {
        "component": "paragraph",
        "text": "These performance overheads are the reason most NoSQL databases, which focus on being lightweight and fast, drop full ACID compliance. And that makes sense actually, the majority of NoSQL use cases involve data where immediate consistency isn‚Äôt critical, such as likes, comment counts, or user profiles. In such scenarios, eventual consistency is perfectly acceptable. "
      },
      {
        "component": "paragraph",
        "text": "\nTo support consistency-sensitive use cases such as financial or healthcare data, MongoDB provides full ACID support by making key architectural changes:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity : MongoDB stores data in BSON format, where each document is a contiguous unit on the disk, making single-document updates atomic. It extends this further with multi-document transactions using Write-Ahead Logging (WAL). While most NoSQL databases (like Cassandra) ensure only document-level atomicity, MongoDB supports atomic operations across multiple documents."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency : Document-level consistency can be enforced through schemas. Note: this is different from the ‚Äúconsistency‚Äù defined in the CAP theorem."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation : MongoDB uses document-level locking for isolation. During transactions, it provides snapshot isolation, ensuring reads within a transaction see a stable view of the data."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability : MongoDB uses a Write-Ahead Log (WAL) called WiredTiger, which provides a configurable property called WriteConcern to manage durability requirements, tunable by the user."
      },
      {
        "component": "paragraph",
        "text": "In short, MongoDB captures the benefits of an ACID-compliant database while keeping overhead minimal, whereas other NoSQL databases often prioritize raw speed and flexibility.\n\n"
      }
    ]
  },
  {
    "id": "25ae9933-5fca-808d-bad4-cf03990771c8",
    "created_time": "2025-08-25T10:12:00.000Z",
    "title": "20 million videos are uploaded to YouTube everyday, Figures for short form video content on instagram and tiktok are beyond comprehension. How do all these platforms handle this massive scale?",
    "date": "2025-08-23",
    "description": [
      {
        "component": "paragraph",
        "text": "When a user uploads a video, the media is chunked on the client. For example, a 200 MB file is split into 40 chunks of 5 MB each before the upload starts. Following that all the chunks are uploaded in parallel to increase upload speed. If a chunk upload fails during the process, only that specific chunk is retried, and once successful, it replaces the incomplete chunk in raw storage."
      },
      {
        "component": "paragraph",
        "text": "The first upload is done to the raw storage until further processing pipelines like transcoding are completed. Raw storage is basically a staging area for the media files and act as temporary buckets. "
      },
      {
        "component": "paragraph",
        "text": "Processing workers pick up the content from queues (Kafka/Pub/Sub) and transcode the videos into multiple bitrates (240p, 480p, 720p)"
      },
      {
        "component": "paragraph",
        "text": "After the processing is done the video chunks are now stored into object storage. Each object is of the format {data(blob), metadata, uniqueId}. Unlike local storage, object storage is designed for distributed environments. It uses a flat, non-hierarchical, key value store architecture, in contrast to the tree structures typically used on top of the local block storage."
      },
      {
        "component": "paragraph",
        "text": "A key benefit of object storage is that it avoids hotspots for popular videos by distributing chunks across nodes, which balances the load during access."
      },
      {
        "component": "paragraph",
        "text": "Videos go through a storage lifecycle. When freshly uploaded, they are pushed to CDNs and hot storage for fast access. As they age and demand drops, they move to cold storage, and eventually end up in archive storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Hot Storage ‚Üí High IOPS (Input/Output per second). Expensive but very fast, typically backed by SSDs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Cold Storage ‚Üí Higher latency, backed by slower HDDs. Cheaper than hot storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Archive Storage ‚Üí Retrieval takes minutes to hours, built on magnetic tapes and deep archive storage infra. Cheapest option."
      }
    ]
  }
]
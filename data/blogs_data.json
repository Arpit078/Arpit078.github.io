[
  {
    "id": "271e9933-5fca-80b1-a60c-fba70a66f8ee",
    "created_time": "2025-09-17T20:09:00.000Z",
    "title": "How do VPN services work?",
    "date": "2025-09-17",
    "description": [
      {
        "component": "paragraph",
        "text": "How a Virtual Private Network works is pretty straightforward and easy to come up with on your own. When you connect to a VPN service using their client you give it permission to capture all of your network requests. The following things happen after your VPN is turned on:"
      },
      {
        "component": "paragraph",
        "text": "➡️ You request a resource from the internet."
      },
      {
        "component": "paragraph",
        "text": "➡️ Your network packets are created as usual, they go through L1 (Application) to L4 (Transport TCP/UDP) layers untouched."
      },
      {
        "component": "paragraph",
        "text": "➡️ In the Network Layer this is where the magic happens. The VPN client creates a virtual network interface. The packet from the previous layer is sent to this virtual network card interface instead of the physical network card. The original packet is now encrypted with a key already shared with the VPN service. The whole packet, with the original IP address it was targeted for, is converted into an encrypted payload for a UDP packet, with the destination address now pointing to the VPN service’s servers."
      },
      {
        "component": "paragraph",
        "text": "➡️ Your packet is now directed to the VPN service’s servers which act as a middleman between you and your desired destination. All your ISP sees is a UDP packet going to the VPN server with some encrypted gibberish content. This is the reason you hear people say with a VPN you are safe in public networks."
      },
      {
        "component": "paragraph",
        "text": "➡️ The VPN server decrypts and creates the original packet with the source destination as its own address, makes the resource request on the internet. The resource provider only sees the IP of the VPN server. The response is received by the VPN server, encrypted, and sent back to your device."
      },
      {
        "component": "paragraph",
        "text": "➡️ The client again captures the packet on L5, decrypts it, and you receive the resource you requested."
      },
      {
        "component": "paragraph",
        "text": "➡️ This extra encryption/decryption step and the middleman architecture is the reason VPNs are generally slower than direct network calls."
      },
      {
        "component": "paragraph",
        "text": "What is hard in this architecture is configuring the virtual network interface. In the summer of last year I was trying to implement my own VPN in Rust, but couldn’t manage the IP tables to route the packets to my tun0 (network interface). There is still an unfinished repository named oss-vpn on my GitHub. There are multiple open-sourced tunneling protocols like OpenVPN, IPSec, WireGuard… I should have learned more about these and used them. Anyways, I came up with managing this exact packet flow using just the Application Layer and created burner browsers. The idea is simple: instead of repackaging on the network layer, change the packets themselves in the application layer. I mean, the goal is to fetch resources out of the internet without exposing your packets, right? Then just spin up the resource-fetching application on the server and the user sends in packets meant for interactions with the resource. To put it simply, there are browser instances on the server which can be viewed by a client using technology such as VNC. Now the problem VPN was trying to solve is somewhat solved. You send interaction packets and receive screen updates. Your local network and ISP see the packets back and forth between the server and you, not the original destination you are consuming resources from. The destination sees traffic from the server’s address, not your address."
      },
      {
        "component": "paragraph",
        "text": "Attaching the repository if it caught your interest. "
      },
      {
        "component": "paragraph",
        "text": "https://github.com/Arpit078/burner-browsers?tab=readme-ov-file#demo"
      }
    ]
  },
  {
    "id": "26fe9933-5fca-8073-a80e-e830a5343285",
    "created_time": "2025-09-15T21:15:00.000Z",
    "title": "How does the driver-matching algorithm work under the hood in ride-sharing apps like Uber and Rapido?",
    "date": "2025-09-16",
    "description": [
      {
        "component": "paragraph",
        "text": "It is a very complex problem that appears simple on the surface. It is hard to understand the matching algorithm even with the simplest constraint, i.e. just minimizing the total wait times. In a real setting there are more factors like reward, ride distance, rider preference(avoid driver that was rated poorly last time), real time issues like traffic, rain and the list goes on…"
      },
      {
        "component": "paragraph",
        "text": "When a user opens the app and sends a request for booking a ride, the first thought might be to simply fetch the driver nearest to the user’s location and assign them. But in a dense area where multiple drivers are available and multiple riders are requesting around the same time, this approach can result in suboptimal allocations."
      },
      {
        "component": "paragraph",
        "text": "For example, suppose you open the app and request a ride. There are two nearby drivers: A → 7 minutes away, and B → 13 minutes away. Now assume another person also requests a ride at about the same time. For that person, the ETAs for the same two drivers are 10 minutes and 26 minutes. If driver A were assigned to you, the other rider would be left with driver B (26 minutes away), leading to a total wait time of 33 minutes. But if driver B were assigned to you, the other rider could get driver A (10 minutes away), giving a total wait time of 23 minutes a significant saving. This makes it clear that a smarter approach is needed, especially with larger pools of drivers and riders."
      },
      {
        "component": "paragraph",
        "text": "As mentioned on Uber’s tech blog, they use this latter approach. Instead of matching each request immediately as it arrives, they batch ride requests for a short duration. This way, we can represent the problem as a bipartite graph: one set represents the riders, the other set represents the drivers, and each edge weight represents the wait time if that driver–rider pair were matched."
      },
      {
        "component": "paragraph",
        "text": "There are multiple algorithms for bipartite matching (a matching is a subset of edges where each node in the graph is connected by at most one edge). This is a very common assignment problem. One brute-force way is to check all n! matchings and pick the one with the minimum total wait time. A more efficient way is to use a polynomial-time algorithm such as the Hungarian algorithm."
      },
      {
        "component": "paragraph",
        "text": "The Hungarian algorithm exploits the structure of the problem, By finding optimal allocations step by step. We create a matrix of riders × drivers, where each cell contains the wait time (edge weight). Then, we reduce the matrix row-wise and column-wise (subtracting the row minimum from each row and the column minimum from each column). This ensures that there is at least one zero-cost pair (rider → driver) that should be part of the optimal matching. If we can find n zero-weight pairs, then we’ve found the optimal assignment. If not, we iteratively adjust the matrix until we reach the optimal assignment. There’s a plethora of resources to learn the full algorithm, and it’s worth learning."
      },
      {
        "component": "paragraph",
        "text": "There is one thing we missed during the discussion, that is the ETA calculation. It is done using geo hashing I covered in one of my previous posts. The process is: find the geohash of the rider, then search for drivers in the same geohash or in neighboring cells. If too many drivers are found, the search space is reduced by increasing geohash precision (adding more characters). This continues until a threshold number of nearby drivers is identified. At that point, the system switches to using precise latitude/longitude coordinates to calculate the actual estimated travel times between drivers and riders."
      },
      {
        "component": "paragraph",
        "text": "That sums it up."
      }
    ]
  },
  {
    "id": "26be9933-5fca-80e0-a739-d16bf3327276",
    "created_time": "2025-09-11T20:20:00.000Z",
    "title": "Why just blocking the IPs won’t help with DDoS…",
    "date": "2025-09-11",
    "description": [
      {
        "component": "paragraph",
        "text": "Been a little busy for the past few days, couldn’t write. So, Distributed Denial of Service (DDoS) on the application? Well, most probably just blocking suspicious IP addresses won’t help. These attacks are designed to come from different source addresses so that there isn’t a quick fix."
      },
      {
        "component": "paragraph",
        "text": "But how can a single attacker have multiple addresses? Well, there are ways. Some operating systems provide ways to interfere with the network layer originally meant for testing, but now used for attacks. The issue could have been resolved if there were mandates for ISPs to discard requests that do not have a source address belonging to their network. But since there is no such mandate, these requests go through. Attackers spoof their IP addresses to flood your servers."
      },
      {
        "component": "paragraph",
        "text": "These attacks target the limited resources of the application server whether by executing high-compute calls like heavy database queries or by flooding the connection pools. Attackers can also use SYN spoofing, which means spoofing TCP connection requests. This exploits the root implementation of the TCP 3-way handshake."
      },
      {
        "component": "paragraph",
        "text": "➡️ A user sends a SYN packet."
      },
      {
        "component": "paragraph",
        "text": "➡️ The server responds with SYN-ACK and stores the user address for retries and connection management."
      },
      {
        "component": "paragraph",
        "text": "➡️ But the attacker’s request does not belong to a legitimate address. The server thinks the request was dropped and retries with the SYN-ACK, but receives no ACK."
      },
      {
        "component": "paragraph",
        "text": "Two things happen:"
      },
      {
        "component": "bulleted_list_item",
        "text": "The connection table fills up because these fake addresses remain there until they time out."
      },
      {
        "component": "bulleted_list_item",
        "text": "The server wastes bandwidth sending requests that go nowhere."
      },
      {
        "component": "paragraph",
        "text": "With the table filling faster and emptying slowly, the server eventually reaches its limit and can’t accept valid requests."
      },
      {
        "component": "paragraph",
        "text": "Apart from IP spoofing, attackers can also use a network of botnets for a distributed attack. In this way, requests are sent from a network of compromised devices — though this is a rarer scenario. "
      },
      {
        "component": "paragraph",
        "text": "Some steps to handle this situation:"
      },
      {
        "component": "paragraph",
        "text": "➡️ On the server firewall, set up rules like limiting the number of open connections a single IP can create, rate-limit the web server, and block IPs that try to establish too many connections in a short span of time or exceed the connection cap. This way, sooner or later, the attacker’s available IPs will be exhausted."
      },
      {
        "component": "paragraph",
        "text": "➡️ In your reverse proxy (NGINX, Apache), configure rate limiting in their config files."
      },
      {
        "component": "paragraph",
        "text": "➡️ Use CDNs like Cloudflare. They offer DDoS protection plans that analyze repeated requests and block them. Since they’re geographically closer to the attacker, they can help detect the region making the exploit requests."
      },
      {
        "component": "paragraph",
        "text": "Real-world DDoS attacks do not always follow a pattern and are much harder to debug. There is no 100% solution against these attacks. But one can plan ahead while developing the application by making design decisions to reduce exposure:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Add CAPTCHAs on the frontend."
      },
      {
        "component": "bulleted_list_item",
        "text": "Rate-limit APIs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Use pagination and other methods to avoid huge DB queries from a single trigger."
      },
      {
        "component": "bulleted_list_item",
        "text": "Use async processing with queues to prevent server overload."
      },
      {
        "component": "paragraph",
        "text": "To read more refer reading Chapter 7 in Computer Security Principles by WILLIAM STALLINGS really good resource."
      }
    ]
  },
  {
    "id": "268e9933-5fca-80de-b031-c76827a2bf8f",
    "created_time": "2025-09-08T20:41:00.000Z",
    "title": "How will you implement a sleep function in javascript",
    "date": "2025-09-08",
    "description": [
      {
        "component": "paragraph",
        "text": "In Python, the sleep function blocks the execution of the current thread for a specified time. Internally, this is implemented by making a system call that suspends the thread, allowing the CPU to be used for other tasks. When the operating system schedules the thread again, it may resume slightly later than the requested interval due to context switching overhead (Something new I learnt today). Therefore, sleep is not guaranteed to be 100% precise."
      },
      {
        "component": "paragraph",
        "text": "In JavaScript, there is no built-in sleep function, and you cannot make direct system calls or suspend the main thread of the browser(since JavaScript runs in a single-threaded event loop in the browser or Node.js)."
      },
      {
        "component": "paragraph",
        "text": "One naive way to mimic sleep would be to use a blocking loop that wastes CPU cycles until the desired time has passed, but this is inefficient and prevents other code from running. This implementation can bring up a disaster when you use this in your frontend code, the whole page will be unresponsive for the interval and might even crash."
      },
      {
        "component": "paragraph",
        "text": "A better approach is to leverage JavaScript’s asynchronous nature. Using setTimeout, one can create a function that returns a Promise which resolves after a delay. Due to the event loop architecture of JS, this does not waste CPU cycles; the program continues to run other tasks like handling the user interactions with the webpage, and once the timer completes, the promise resolves and execution resumes at the next .then or await with the next line of code after your sleep."
      },
      {
        "component": "paragraph",
        "text": "Though not the same thread blocking implementation as of the sleep function in python. Async sleep gets the job done."
      }
    ]
  },
  {
    "id": "265e9933-5fca-807b-b389-f6ea98071a83",
    "created_time": "2025-09-05T17:34:00.000Z",
    "title": "How do unordered hashmaps work?",
    "date": "2025-09-05",
    "description": [
      {
        "component": "paragraph",
        "text": "Unordered hashmaps are a bunch of array buckets mapped to keys using hashing functions, nothing more really."
      },
      {
        "component": "paragraph",
        "text": "A simple hashing function works by taking every byte of the key, XORing it with a very large offset, then multiplying the result with a very large prime number. Keep repeating this with all of its characters until the final hash is obtained. This is called the FNV-1a algorithm, commonly used in C++ for hashing strings. Then the index of the key is calculated using:"
      },
      {
        "component": "paragraph",
        "text": "index = hash(key) % capacity"
      },
      {
        "component": "paragraph",
        "text": "That’s it for the insertion part."
      },
      {
        "component": "paragraph",
        "text": "Now there is a possibility of collision — different keys mapping to the same index. There are two methods to handle collisions in these scenarios:"
      },
      {
        "component": "numbered_list_item",
        "text": "Chaining – The array buckets/slots are containers like linked lists or vectors. These can then chain the key-value pairs that fall on the same index. For example, on a bucket you might see something like:"
      },
      {
        "component": "numbered_list_item",
        "text": "Open Addressing – In this approach, when collisions happen, instead of storing in the same slot, another empty slot is found. This can be done by linearly searching for the next vacant bucket, or using a probing function to find a new bucket, or by rehashing the key with a second hash function. While querying, you first get the index with the first hash function if the key matches, done. Otherwise, you follow the probing sequence (linear/quadratic/double hashing) until you either find the key or hit an empty slot."
      },
      {
        "component": "paragraph",
        "text": "That’s about it."
      }
    ]
  },
  {
    "id": "264e9933-5fca-804d-af9c-c2efe62c453e",
    "created_time": "2025-09-04T11:59:00.000Z",
    "title": "How does Live broadcast work?",
    "date": "2025-09-04",
    "description": [
      {
        "component": "paragraph",
        "text": "That live cricket match you saw on jiohotstar …. how does that work?"
      },
      {
        "component": "paragraph",
        "text": "The broadcaster sends a compressed video stream to the ingest edge server. The video sent by the source is a continuous stream over a TCP connection. It is compressed and usually has a single bitrate/resolution. The video is then uncompressed/decoded into raw image frames, and the audio is extracted."
      },
      {
        "component": "paragraph",
        "text": "The stream is encoded into different resolutions and bitrates, and similarly for the audio. Encoders use GOPs (Groups of Pictures), for example, 2-second chunks. As soon as the first GOP of raw frames is processed, the transcoder can output it. This means you don’t wait for the whole stream; instead, you process in a rolling window, per GOP. The transcoding happens in parallel for all the resolutions of the current GOP. The encoder-generated files do not contain any metadata for the video stream (such as length, bitrate, or resolution), so packaging is required for delivery to CDNs and the origin storage bucket."
      },
      {
        "component": "paragraph",
        "text": "The packager takes each GOP and writes it into a small file called a segment. For each rendition (encoder output at a different resolution), a playlist (manifest) is generated. A playlist is essentially a text metadata file that defines the segment order and details of the different renditions. There are two major protocols for streaming content over HTTP:"
      },
      {
        "component": "paragraph",
        "text": "HLS (HTTP Live Streaming)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Uses .m3u8 (text-based playlists)."
      },
      {
        "component": "bulleted_list_item",
        "text": "Segments are usually MPEG-TS (.ts) or fMP4 (.m4s)."
      },
      {
        "component": "paragraph",
        "text": "MPEG-DASH (Dynamic Adaptive Streaming over HTTP)"
      },
      {
        "component": "bulleted_list_item",
        "text": "An open standard."
      },
      {
        "component": "bulleted_list_item",
        "text": "Uses .mpd files (XML-based manifests)."
      },
      {
        "component": "bulleted_list_item",
        "text": "Segments are usually fMP4 (.m4s)."
      },
      {
        "component": "paragraph",
        "text": "The packager emits sub-GOP span chunks, around 200–400 ms in length, compared to the 2-second GOPs received from the encoder. Multiple of these smaller chunks are delivered to the CDN along with the updated playlists. This is done to enable faster downloads on the client."
      },
      {
        "component": "paragraph",
        "text": "Finally, the client player periodically requests the playlists according to the resolution supported by the client’s network. The corresponding video segments are downloaded and played live on the client, typically with a 2–3 second delay from when the original event was recorded."
      }
    ]
  },
  {
    "id": "263e9933-5fca-80a2-a855-dc4f4e07d9d1",
    "created_time": "2025-09-03T19:20:00.000Z",
    "title": "How is code sandboxing on platforms like LeetCode designed?",
    "date": "2025-09-03",
    "description": [
      {
        "component": "paragraph",
        "text": "It is a very sensitive part of the design for coding platforms like LeetCode and GeeksforGeeks. Allowing users to send in code and run it on your server is a dangerous choice. On the surface, it just looks like running Docker containers for execution, but that too needs a lot of modifications since it shares the kernel with the host machine."
      },
      {
        "component": "paragraph",
        "text": "Option 1: Docker containers"
      },
      {
        "component": "bulleted_list_item",
        "text": "Prebuilt language-specific containers are used to execute code in a sandbox."
      },
      {
        "component": "bulleted_list_item",
        "text": "They use namespaces like the process namespace that isolates the container processes, and the network namespace which disables any network access to the host."
      },
      {
        "component": "bulleted_list_item",
        "text": "cgroups are used to put resource consumption limits on the processes of these containers. For example, for every test case, max use 2s or 256MB RAM."
      },
      {
        "component": "bulleted_list_item",
        "text": "seccomp is added to protect against any system call generated by the user code. Filesystem access is controlled for the code execution, and code can only access the memory it needs for fair use."
      },
      {
        "component": "bulleted_list_item",
        "text": "Though a lot of security modifications are needed, it has benefits, to begin with, it is faster to start up and has low memory overhead."
      },
      {
        "component": "bulleted_list_item",
        "text": "The downside is the kernel is shared; if a kernel escape exploit is found, the host machine can be compromised."
      },
      {
        "component": "bulleted_list_item",
        "text": "Preferable for low-risk loads. Suitable for interpreted languages where dangerous syscalls are either not present (like no mount() syscall in Python) or are made via the interpreter."
      },
      {
        "component": "paragraph",
        "text": "Option 2: Firecracker MicroVMs"
      },
      {
        "component": "bulleted_list_item",
        "text": "Lightweight virtual machines, open-sourced and heavily used and developed by AWS."
      },
      {
        "component": "bulleted_list_item",
        "text": "Startups are fast but slower compared to Docker containers."
      },
      {
        "component": "bulleted_list_item",
        "text": "Stronger isolation though, as the kernel is its own, not shared. Any syscall stays in the VM and does not reach the host machine."
      },
      {
        "component": "bulleted_list_item",
        "text": "Strongly bounded memory, as a complete OS is running and cannot read memory outside of it."
      },
      {
        "component": "bulleted_list_item",
        "text": "Usable for high-risk loads like compiled languages such as C++ and Rust, where direct syscalls can be made by the compiled machine code."
      },
      {
        "component": "paragraph",
        "text": "Platforms like LeetCode use a hybrid model for code sandboxing. For trusted clients and loads, they use Docker-based containers, while for risky and untrusted submissions, microVMs are used."
      },
      {
        "component": "paragraph",
        "text": "Question…AWS Fargate uses Firecracker underneath and Google Cloud Run uses gVisor (another virtualization tech with user-space kernels, giving more isolation and performance in between Firecracker and Docker containers). They are good with isolation, so why do these platforms not use services like these?"
      },
      {
        "component": "paragraph",
        "text": "These serverless products are designed for long-running tasks like APIs and workers. While for online judge architecture you have short-lived and bulk tasks. Code is executed in seconds and then it is over. If one uses these products, they would need to spin up a container for every submission, which, not to be ignored, is far slower than a native Docker container spin-up or Firecracker startup. Another problem is that very fine-grained control is required on resource consumption with user-submitted programs, i.e., max 2s per test case or 256MB max RAM usage. While in Fargate or Cloud Run you get to manage the container size and memory limit, you can’t set up cgroup limits like we discussed in the earlier part. And serverless is 10x more costly than running your own orchestration of containers and microVMs."
      }
    ]
  },
  {
    "id": "262e9933-5fca-80e5-8772-c7a7e5c70491",
    "created_time": "2025-09-02T19:13:00.000Z",
    "title": "Tech behind torrents",
    "date": "2025-09-02",
    "description": [
      {
        "component": "paragraph",
        "text": "I’ve always been fascinated by the tech behind torrents, no central server hosting files, and yet files can download at lightning speeds. There’s no single server keeping track of which peer has which piece, but somehow the whole file gets downloaded. How does it all work?"
      },
      {
        "component": "paragraph",
        "text": "Torrents operate on a P2P (peer-to-peer) file sharing network. Instead of transferring one giant file, the content is broken down into many smaller pieces."
      },
      {
        "component": "paragraph",
        "text": "The .torrent file contains hashes for these pieces, which serve two purposes:"
      },
      {
        "component": "bulleted_list_item",
        "text": "They tell the client which pieces to download."
      },
      {
        "component": "bulleted_list_item",
        "text": "They verify file integrity, ensuring that downloaded pieces haven’t been tampered with."
      },
      {
        "component": "paragraph",
        "text": "In a torrent network, there are two main types of users:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Leechers: users who are downloading file pieces."
      },
      {
        "component": "bulleted_list_item",
        "text": "Seeders: users who are uploading file pieces."
      },
      {
        "component": "paragraph",
        "text": "A single user can be both at once, as soon as you download a piece, you can immediately begin uploading it to others."
      },
      {
        "component": "paragraph",
        "text": "To download pieces, your client needs to know where peers are. In the early days, torrents relied heavily on a central tracker server that acted like a directory, keeping a list of which users had which files."
      },
      {
        "component": "paragraph",
        "text": "Modern torrent clients, however, are more decentralized. While trackers still exist, they’re no longer the only way to find peers. Instead, clients use a Distributed Hash Table (DHT):"
      },
      {
        "component": "paragraph",
        "text": "Once you open your client it goes to the tracker to get the addresses of some nearby peers. Every peer stores a local Distributed hash table, which contains address:port → info hashes of files that can be downloaded from that peer. From one peer to another the search goes on until you get the nearest peer that has the file you are looking for. And with that one downloads the whole file without relying on a single server."
      }
    ]
  },
  {
    "id": "261e9933-5fca-8021-9db8-c34dc95f1473",
    "created_time": "2025-09-01T19:18:00.000Z",
    "title": "What are SOAP and GraphQL",
    "date": "2025-09-01",
    "description": [
      {
        "component": "paragraph",
        "text": "REST is so commonplace that we never dig deeper to learn that there other ways of designing APIs."
      },
      {
        "component": "paragraph",
        "text": "🔹 GraphQL"
      },
      {
        "component": "bulleted_list_item",
        "text": "GraphQL is a query language for APIs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Instead of the server deciding what data you get (like in REST) the client asks exactly what it needs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Instead of multiple endpoints, there is only one flexible endpoint."
      },
      {
        "component": "bulleted_list_item",
        "text": "You just define a schema and data resolvers(methods to fetch the data), the client sends in dynamic requests around the schema with what it needs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Like you can get {“query” : \"{ user(id: 123) { name email orders { id total } } }”} no specific handler needs to be coded on the backend for different queries."
      },
      {
        "component": "bulleted_list_item",
        "text": "This reduces over-fetching/under-fetching problems common with REST. Just return exactly what was asked."
      },
      {
        "component": "paragraph",
        "text": "🔹 SOAP (Simple Object Access Protocol)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Heavily used in enterprise and legacy systems (e.g., banking, telecom, government)."
      },
      {
        "component": "bulleted_list_item",
        "text": "Operates over multiple protocols (HTTP, SMTP, etc.), not just HTTP."
      },
      {
        "component": "bulleted_list_item",
        "text": "Uses XML as request/response objects. Has a strict schema defined for the objects. If req/res does not follow the schema, the API call fails. "
      },
      {
        "component": "bulleted_list_item",
        "text": "Comes with strict standards and built-in security (WS-Security), Portions like certain fields of the req/res object can be encrypted separately in the application layer itself! while in REST it is done in the Transport layer."
      },
      {
        "component": "bulleted_list_item",
        "text": "Many banking systems use SOAP because it provides stricter control over payloads and is generally considered more secure."
      }
    ]
  },
  {
    "id": "260e9933-5fca-8052-8b02-d0f38eb99378",
    "created_time": "2025-08-31T18:14:00.000Z",
    "title": "What are preflight requests and why are they important?",
    "date": "2025-08-31",
    "description": [
      {
        "component": "paragraph",
        "text": "No, they are not redundant. Remember last time you set Access-Control-Allow-Origin = \"*\"  in your backend? Well, that can be a serious problem!"
      },
      {
        "component": "paragraph",
        "text": "A preflight request is a type of request that web browsers send before making a cross-origin HTTP request. This is part of the CORS (Cross-Origin Resource Sharing) mechanism."
      },
      {
        "component": "paragraph",
        "text": "Browsers send preflight requests before any request that is considered non-simple. Non-simple requests include:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Methods like PUT, DELETE, or PATCH."
      },
      {
        "component": "bulleted_list_item",
        "text": "POST requests with content types other than application/x-www-form-urlencoded, multipart/form-data, or text/plain."
      },
      {
        "component": "bulleted_list_item",
        "text": "Requests with custom headers (e.g., Authorization, X-API-KEY, etc.)."
      },
      {
        "component": "paragraph",
        "text": "Preflight requests use the OPTIONS HTTP method to check with the backend server whether the actual request is allowed from the requesting origin. Only if the backend responds with the proper CORS headers (essentially saying the origin is allowed) will the browser proceed with the real request."
      },
      {
        "component": "paragraph",
        "text": "This is especially important when you use cookies for authentication in your backend. When a website makes a request to a server, the browser automatically includes the cookies associated with that server’s origin. Even though frontend JavaScript cannot access cookies from other origins or tabs, the browser attaches them to outgoing requests for that origin, for authentication purpose of that request."
      },
      {
        "component": "paragraph",
        "text": "If preflight checks didn’t exist, or if the backend allowed all origins (*), then any malicious website could send requests to your backend from the victim’s browser. Since the browser would automatically attach authentication cookies, the attacker could perform actions on behalf of the logged-in user on you backend. This kind of exploit is called CSRF (Cross-Site Request Forgery)."
      },
      {
        "component": "paragraph",
        "text": "Preflight requests help prevent this by ensuring that only trusted origins are allowed to perform non-simple requests. They cannot be bypassed in a normal browser environment. Preflight requests are important aren’t they?"
      }
    ]
  },
  {
    "id": "25fe9933-5fca-809a-bba4-e928fc2db523",
    "created_time": "2025-08-30T19:43:00.000Z",
    "title": "How is a rate limiter designed for distributed environments?",
    "date": "2025-08-30",
    "description": [
      {
        "component": "paragraph",
        "text": "Probably, this will be something more than what you’ve previously read about the topic."
      },
      {
        "component": "paragraph",
        "text": "Most commonly, a rate limiter is implemented in an API Gateway. An API Gateway is essentially a reverse proxy with fully managed services like rate limiting, SSL termination, authentication, IP whitelisting, and static content handling. It can convert request/response object formats (e.g., XML → JSON) or perform authentication for user requests without ever hitting your backend service. Some commonly used API gateways are AWS API Gateway, Kong Gateway and Apigee. Rate limiting can also be implemented with the backend services without an API Gateway, depending on product requirements."
      },
      {
        "component": "paragraph",
        "text": "There are multiple algorithms used for rate limiting, but the most common are Token Bucket and Leaky Bucket. The idea is pretty straightforward."
      },
      {
        "component": "paragraph",
        "text": "In the Token Bucket algorithm, each user has a bucket of tokens that is refilled at a constant rate. When a user request arrives, if a token is present in the bucket, the request proceeds; otherwise, it is dropped. This is the easy part. The harder part is the refilling process. For millions of users, there would need to be millions of refilling processes accessing a shared Redis backend, which would result in a nightmare of Redis requests."
      },
      {
        "component": "paragraph",
        "text": "A solution to this is to remove the explicit refiller. Instead, you store some additional information the last access timestamp. For example, you store:"
      },
      {
        "component": "paragraph",
        "text": "user123 : { tokens_left, last_access_timestamp }"
      },
      {
        "component": "paragraph",
        "text": "When a new request comes in, you calculate how many tokens would have been refilled since the last access and add that to tokens_left. Based on this, you can decide whether the request is allowed."
      },
      {
        "component": "paragraph",
        "text": "However, this approach has a caveat in distributed systems. It’s no longer just a simple counter increment, it requires calculation, which means a two-way round trip to the server. First, the server reads the last timestamp from Redis, then calculates the updated token count, and finally writes it back. This can create race conditions."
      },
      {
        "component": "paragraph",
        "text": "For example, suppose two push notifications need to be sent to user123 at exactly the same time, and they are handled by two different nodes. Node A fetches the last timestamp, calculates the updated value, and prepares to update Redis. Meanwhile, Node B fetches the now stale timestamp and performs its own calculation. This causes inconsistencies and may occasionally allow requests to go over the limit."
      },
      {
        "component": "paragraph",
        "text": "The final solution to this problem is to store only the access timestamps in a sorted set in Redis. The set stores only a range of timestamps — for example, at most the last 3 timestamps within the last 10 seconds. The size of the set then acts as the counter. A new timestamp is added only if the set size is below the limit; otherwise, the request is dropped. This can be done in one atomic transaction using built-in Redis functions such as:"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZREMRANGEBYSCORE (remove elements outside the time range)"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZADD (add the new timestamp)"
      },
      {
        "component": "bulleted_list_item",
        "text": "ZCARD (count the size)"
      },
      {
        "component": "paragraph",
        "text": "A question….if Redis can do all this, why can’t it also calculate the token refill (tokens_left + (current_timestamp - last_timestamp)/rate) in a single transaction? The reason is that Redis has no built-in function for this calculation, so backend logic is still needed. However, this can be solved by using Redis Lua scripts, which allows you to define custom logic and execute it atomically. That way, the Token Bucket approach can also work in one single atomic transaction!"
      },
      {
        "component": "paragraph",
        "text": "Cool reference article for this : https://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/"
      }
    ]
  },
  {
    "id": "25ee9933-5fca-80c1-b06b-d613f8fb768f",
    "created_time": "2025-08-29T17:40:00.000Z",
    "title": "What happens in the backend when you do a UPI transaction?",
    "date": "2025-08-29",
    "description": [
      {
        "component": "paragraph",
        "text": "When you do a UPI transaction, the payment app extracts the Virtual Payment Address (VPA or UPI ID) from the QR code or the contact number. Payment apps like Paytm, GPay, and PhonePe are called Payment Service Provider (PSP) apps. The transaction request is sent to the NPCI (National Payments Corporation of India), which maintains a directory that maps VPAs to bank accounts."
      },
      {
        "component": "paragraph",
        "text": "The request is routed to the issuer bank (payer’s bank). It validates the request using the UPI PIN and the payer’s account balance."
      },
      {
        "component": "paragraph",
        "text": "The bank then sends a debit confirmation to the NPCI UPI switch, which routes a credit request to the receiver’s bank."
      },
      {
        "component": "paragraph",
        "text": "The settlement in the banks reflects instantly for the user, but the actual money transfer between banks happens through the RBI’s payment system. After regular intervals, a net transaction amount is calculated and fund transfers are settled between banks."
      },
      {
        "component": "paragraph",
        "text": "One important aspect of this whole setup is security. When you enter the UPI PIN, you do it through a secure keypad designed with additional safeguards. The keypad is a very peculiar case it is not a component of the payment app instead, it is provided by the NPCI’s UPI SDK."
      },
      {
        "component": "paragraph",
        "text": "This is an isolated UI component—no input is registered through the standard input channels of Android or iOS that are used by regular keyboard apps. As soon as a keystroke is registered, the digit is immediately encrypted by the SDK. The UPI PIN never exists in memory as plain text. Every input block is encrypted using the public key provided by the payer’s bank. The corresponding private key is securely stored only with the bank. Not even NPCI has access to your PIN."
      },
      {
        "component": "paragraph",
        "text": "Even banks do not store the PIN in plain text. They use symmetric encryption to encrypt the PIN and store it inside a Hardware Security Module (HSM), a tamper-proof device that self-erases data if tampering is attempted. The PIN entered by the user is decrypted using the private key, re-encrypted inside the HSM using the symmetric key, and then compared against the stored encrypted block."
      },
      {
        "component": "paragraph",
        "text": "That’s a lot going for a transaction that happens almost instantly!"
      }
    ]
  },
  {
    "id": "25de9933-5fca-8075-a89a-c15181577fe4",
    "created_time": "2025-08-28T16:01:00.000Z",
    "title": "Proxy, Forward Proxy and Reverse Proxy what are they?",
    "date": "2025-08-28",
    "description": [
      {
        "component": "paragraph",
        "text": "A Proxy is basically a layer that sits between the server and the client. Whenever a request–response cycle takes place in your application, it goes through the proxy. One might find a lot of use cases as to why a proxy might be required, and depending on the use case, proxies can be broadly divided into two categories: Forward Proxy and Reverse Proxy."
      },
      {
        "component": "paragraph",
        "text": "🔹 Forward Proxy"
      },
      {
        "component": "paragraph",
        "text": "A forward proxy sits in front of the client. When the client sends a request, it first passes through the forward proxy before reaching the server. The server sees the proxy as the requester, not the actual client."
      },
      {
        "component": "paragraph",
        "text": "Common use cases:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Enforcing corporate/institutional policies (e.g., blocking certain websites)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Anonymizing client identity, So all the server sees is the ip address of the proxy server requesting the resource"
      },
      {
        "component": "bulleted_list_item",
        "text": "Forward proxy acts as an abstraction layer for the clients."
      },
      {
        "component": "paragraph",
        "text": "🔹 Reverse Proxy"
      },
      {
        "component": "paragraph",
        "text": "A reverse proxy, on the other hand, sits in front of the server. The client makes a request, but it only communicates with the reverse proxy, which then forwards the request to the appropriate server on behalf of the client. Here, the client never directly interacts with the actual server."
      },
      {
        "component": "paragraph",
        "text": "Common use cases:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Load balancing across multiple servers."
      },
      {
        "component": "bulleted_list_item",
        "text": "Routing requests to different services, i.e. yourdomain.com/service1 and service2 maybe running on the same server but on different ports, proxy server can direct the traffic accordingly."
      },
      {
        "component": "bulleted_list_item",
        "text": "Enhancing security by hiding server details(Abstraction layer for the server)"
      },
      {
        "component": "paragraph",
        "text": "In short:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Forward Proxy → client-facing (protects/manages the client)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Reverse Proxy → server-facing (protects/manages the server)"
      }
    ]
  },
  {
    "id": "25ce9933-5fca-805b-8041-df7e15ca1191",
    "created_time": "2025-08-27T17:38:00.000Z",
    "title": "How do food delivery apps like swiggy, zomato find the nearest restaurants in milliseconds",
    "date": "2025-08-27",
    "description": [
      {
        "component": "paragraph",
        "text": "When you start thinking about it, the problem looks very simple: just store the restaurants’ coordinates, and once the user opens the app, calculate the distance to the restaurants, sort them, and return the results. Easy, right? Yes, until your app grows beyond being a pet project. When the user base reaches millions or even billions, optimized approaches are required."
      },
      {
        "component": "paragraph",
        "text": "One way to handle this is by using geo-enabled databases and spatial indexing."
      },
      {
        "component": "paragraph",
        "text": "For example, insert your restaurants into a geo-enabled database like PostGIS (Postgres) or MongoDB. Then build spatial indexes on the database. Internally, these use R-Trees (rectangle trees), where nearby restaurants are stored within the same rectangles."
      },
      {
        "component": "paragraph",
        "text": "When a user opens the app at a given location (lat, lon), the backend runs a query with a radius (e.g., all restaurants within 5 km). The index filters out relevant restaurants using bounding rectangles with the user location rectangle, and then the exact distance is calculated for the candidates. This is a workable approach when the number of rows is in the millions."
      },
      {
        "component": "paragraph",
        "text": "However, when dealing with billions of rows, geohashing is often used."
      },
      {
        "component": "paragraph",
        "text": "In this setup, the world is divided into grid cells, and each cell is assigned a unique ID (a geohash). Locations that are close to each other have similar geohash prefixes. The algorithm for computing a geohash is straightforward:"
      },
      {
        "component": "numbered_list_item",
        "text": "Take the user’s longitude and latitude. The longitude range is [-180, 180] and the latitude range is [-90, 90]."
      },
      {
        "component": "numbered_list_item",
        "text": "Create a bit array (size depends on the precision required). Each geohash character requires 5 bits."
      },
      {
        "component": "numbered_list_item",
        "text": "Start with the longitude: if it lies in the upper half of the range, set the bit; otherwise, leave it unset."
      },
      {
        "component": "numbered_list_item",
        "text": "Halve the range (e.g., [0, 180]) and do the same for latitude to set the next bit."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat this process until the array is filled."
      },
      {
        "component": "numbered_list_item",
        "text": "Take 5 bits at a time, convert them to base32, and map them to a character from '0123456789bcdefghjkmnpqrstuvwxyz'."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat until you obtain the final geohash string."
      },
      {
        "component": "paragraph",
        "text": "The more characters in the geohash, the more precise the location. For example:"
      },
      {
        "component": "bulleted_list_item",
        "text": "5 characters → ~4.9 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "6 characters → ~1.2 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "7 characters → ~152 m precision"
      },
      {
        "component": "paragraph",
        "text": "Food delivery apps usually use 6–7 characters for querying. The query simply looks for restaurants with a geohash equal to the user’s geohash or its neighbors. This is a significantly faster approach than calculating distances for every restaurant in the database. 😂"
      }
    ]
  },
  {
    "id": "25be9933-5fca-8026-8dbd-c96403b39a0c",
    "created_time": "2025-08-26T16:34:00.000Z",
    "title": "Elasticsearch: What is it really, and why is it so fast?",
    "date": "2025-08-26",
    "description": [
      {
        "component": "paragraph",
        "text": "Elasticsearch is a search engine program that sits on top of your database to make your search queries blazingly fast. Where SQL might take minutes Elasticsearch responds within milliseconds."
      },
      {
        "component": "paragraph",
        "text": "It supports full text searches, aggregations, geo queries, autocompletes and a lot more. "
      },
      {
        "component": "paragraph",
        "text": "The architecture involved in setting up Elasticsearch system is as follows : "
      },
      {
        "component": "bulleted_list_item",
        "text": "Data source - The source of Truth : Postgres, MySQL, MongoDB"
      },
      {
        "component": "bulleted_list_item",
        "text": "Ingestion pipeline- To  copy or stream the data into Elasticsearch from the database."
      },
      {
        "component": "bulleted_list_item",
        "text": "Elasticsearch cluster - Multiple nodes with primary and replica shards of indexes(A big Index over the whole database is broken down into multiple smaller shards so that you can query all of the index in one go with parallel searches on these shards pretty clever!)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Search API - search queries are done via REST API (JSON over HTTP)"
      },
      {
        "component": "paragraph",
        "text": "Okay Elasticsearch is great and all but how does it do it?"
      },
      {
        "component": "paragraph",
        "text": "A very rudimentary view of the inner workings can be taken by the following illustration :"
      },
      {
        "component": "paragraph",
        "text": "1️⃣ Data ingestion in the Elasticsearch - data is inserted using POST API"
      },
      {
        "component": "paragraph",
        "text": "POST /book/_doc/1 "
      },
      {
        "component": "paragraph",
        "text": "{“title” : “Harry  Potter”, “author” : “J. K. Rowling”, “year” : 1997}"
      },
      {
        "component": "paragraph",
        "text": "2️⃣ Inverted indexes are created - Think of these indexes as the word indexes found at the end of books. Tokens are mapped to the documents they occurred in."
      },
      {
        "component": "paragraph",
        "text": "“Harry” → Doc1"
      },
      {
        "component": "paragraph",
        "text": "“Potter” → Doc1"
      },
      {
        "component": "paragraph",
        "text": "“Hogwarts” → Doc2"
      },
      {
        "component": "paragraph",
        "text": "3️⃣ Scoring and Ranking for search - Search inputs are broken into tokens and matched with the relevant documents using the Inverted Indexes. Docs having the highest occurrence of the token are given higher scores. Docs which contain the rarer word are given higher score(”Hogwarts” is rarer than “the”). The higher the score the higher the rank."
      },
      {
        "component": "paragraph",
        "text": "4️⃣ Indexes are broken into smaller shards and spread across multiple nodes, for faster lookups."
      },
      {
        "component": "paragraph",
        "text": "One more notable property of Elasticsearch is that it stores the fields in a columnar fashion. Which significantly boosts the aggregation speeds. Now to get averages instead of going to multiple rows you are only required to traverse all the entries in the column."
      },
      {
        "component": "paragraph",
        "text": "What it looks like is : "
      },
      {
        "component": "paragraph",
        "text": "Field : year → [1997 2005 1996]"
      },
      {
        "component": "paragraph",
        "text": "Field : prices → [20 15 25] "
      },
      {
        "component": "paragraph",
        "text": "That sums up, absolute basics one should know about Elasticsearch."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-80d3-9531-cab57ede7228",
    "created_time": "2025-08-25T14:42:00.000Z",
    "title": "Bloom Filters? You've probably read the name somewhere.",
    "date": "2025-08-25",
    "description": [
      {
        "component": "paragraph",
        "text": "Bloom filter is a data structure used to test whether an element is a member of a set or not. It is probabilistic in nature, what that means is that sometimes it may return false positives (i.e., it may return true even if the element does not exist in the set), while it never returns a false negative. If the bloom filter says the element is not a member then it is not a member 100%. It trades a little bit of accuracy for big gains in speed and memory efficiency."
      },
      {
        "component": "paragraph",
        "text": "Bloom filters are very handy in large scale systems like in key value stores, distributed HashMap where filtering out which node might contain the element saves a lot of time."
      },
      {
        "component": "paragraph",
        "text": "The implementation for bloom filters is very easy to be honest, all you need is a bit array and some hash functions. Let’s take an example :"
      },
      {
        "component": "paragraph",
        "text": "Consider a bit array of size 10 = [0 0 0 0 0 0 0 0 0 0] and 2 hash functions H1 and H2"
      },
      {
        "component": "paragraph",
        "text": "Let’s insert a key “cat” in our filter :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(”cat”) = 2; idx2 =  H2(”cat”) = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "Set all the bits at indexes idx1 and idx2 in the bit array."
      },
      {
        "component": "bulleted_list_item",
        "text": "Updated bit array : [0 0 1 0 0 0 0 1 0 0]"
      },
      {
        "component": "paragraph",
        "text": "We’ve inserted the key."
      },
      {
        "component": "paragraph",
        "text": "Now to query a key “dog” follow these steps :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(”dog”) = 4; idx2 = H2(”dog”)  = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "check all the bits in the bit array at the idx1 and idx2."
      },
      {
        "component": "bulleted_list_item",
        "text": "If all the bits are set the element is in the set else not. Clearly dog is not a member."
      },
      {
        "component": "paragraph",
        "text": "This procedure of querying explains the reason for false positives as there might be collisions, the bits might be set by some other key but match the same pattern of another key not in the set."
      },
      {
        "component": "paragraph",
        "text": "In production the size of the bit arrays is reasonable, for storing 1 Million items the size required is 9.2 Million bits roughly 1.2 MB and 7 hash functions. The setup responds with 1% of false positives."
      },
      {
        "component": "paragraph",
        "text": "When not to use a Bloom filter (important to know)"
      },
      {
        "component": "paragraph",
        "text": "1️⃣ When false positives are not allowed. Like Checking if a password hash exists in a breach database. A false positive could wrongly tell a user their password is compromised."
      },
      {
        "component": "paragraph",
        "text": "2️⃣ When the dataset keeps on changing frequently, because bloom filters do not provide a method for key deletion as two or more keys might have set a common bit. But aren’t key value stores meant to be constantly changing datasets? Well yes they are! but they have a very clever solution in place which can be discussed in some other article."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-8076-97c5-c03d05e7e1fb",
    "created_time": "2025-08-25T10:13:00.000Z",
    "title": "You heard MongoDB follows ACID properties while other NoSQL DBs don’t. Do you know How and Why?",
    "date": "2025-08-24",
    "description": [
      {
        "component": "paragraph",
        "text": "TL;DR : NoSQL databases are BASE(Basically Available Soft State Eventually Consistent) by choice not by any limitation to NoSQL data storage format. ACID databases have a very specific use case in scenarios where consistency can’t be compromised say for Banking applications, Healthcare data storage. MongoDB tries to enter these use cases by being ACID. "
      },
      {
        "component": "paragraph",
        "text": "Long Answer : Unlike SQL databases, which were designed for single-node architectures, NoSQL databases must prioritize availability across multiple nodes. At scale, partition tolerance is a necessity if there’s a network outage between nodes, the database should still function. To achieve this, many NoSQL systems choose to be AP (Available and Partition-tolerant) as per the CAP theorem, since they cannot be fully consistent and AP at the same time."
      },
      {
        "component": "paragraph",
        "text": "But this still doesn’t explain why most NoSQL databases aren’t ACID. The term ACID, made popular by SQL databases, requires that a compliant database ensures:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity: In a transaction either all the operations are successful or none of them are."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency: After a transaction the DB should remain in a valid state, always respecting constraints such as schemas."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation: Every transaction should happen in Isolation, such that no other transaction is affected. This is often implemented using MVCC (Multi-Version Concurrency Control), which I’ll cover later."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability: Once committed, a transaction is permanent and crash-safe, often implemented via Write-Ahead Logging (WAL), also discussed later."
      },
      {
        "component": "paragraph",
        "text": "By definition ACID appears a transaction specific property, no relationship with what it has to do with the database being SQL or NoSQL."
      },
      {
        "component": "paragraph",
        "text": "First thing an ACID database should have atomicity and durability which is often implemented using Write ahead logging(WAL) = a special sequential log file where all the changes are written before they are applied to the actual database. Secondly for Isolation, typically achieved through Multi version concurrency Control(MVCC) Instead of overwriting data mid-transaction, the database maintains multiple versions: one before the transaction and one after it completes. This ensures other operations aren’t affected by an incomplete transaction. A less common alternative is to enforce isolation using locking mechanisms."
      },
      {
        "component": "paragraph",
        "text": "These performance overheads are the reason most NoSQL databases, which focus on being lightweight and fast, drop full ACID compliance. And that makes sense actually, the majority of NoSQL use cases involve data where immediate consistency isn’t critical, such as likes, comment counts, or user profiles. In such scenarios, eventual consistency is perfectly acceptable. "
      },
      {
        "component": "paragraph",
        "text": "\nTo support consistency-sensitive use cases such as financial or healthcare data, MongoDB provides full ACID support by making key architectural changes:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity : MongoDB stores data in BSON format, where each document is a contiguous unit on the disk, making single-document updates atomic. It extends this further with multi-document transactions using Write-Ahead Logging (WAL). While most NoSQL databases (like Cassandra) ensure only document-level atomicity, MongoDB supports atomic operations across multiple documents."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency : Document-level consistency can be enforced through schemas. Note: this is different from the “consistency” defined in the CAP theorem."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation : MongoDB uses document-level locking for isolation. During transactions, it provides snapshot isolation, ensuring reads within a transaction see a stable view of the data."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability : MongoDB uses a Write-Ahead Log (WAL) called WiredTiger, which provides a configurable property called WriteConcern to manage durability requirements, tunable by the user."
      },
      {
        "component": "paragraph",
        "text": "In short, MongoDB captures the benefits of an ACID-compliant database while keeping overhead minimal, whereas other NoSQL databases often prioritize raw speed and flexibility.\n\n"
      }
    ]
  },
  {
    "id": "25ae9933-5fca-808d-bad4-cf03990771c8",
    "created_time": "2025-08-25T10:12:00.000Z",
    "title": "20 million videos are uploaded to YouTube everyday, Figures for short form video content on instagram and tiktok are beyond comprehension. How do all these platforms handle this massive scale?",
    "date": "2025-08-23",
    "description": [
      {
        "component": "paragraph",
        "text": "When a user uploads a video, the media is chunked on the client. For example, a 200 MB file is split into 40 chunks of 5 MB each before the upload starts. Following that all the chunks are uploaded in parallel to increase upload speed. If a chunk upload fails during the process, only that specific chunk is retried, and once successful, it replaces the incomplete chunk in raw storage."
      },
      {
        "component": "paragraph",
        "text": "The first upload is done to the raw storage until further processing pipelines like transcoding are completed. Raw storage is basically a staging area for the media files and act as temporary buckets. "
      },
      {
        "component": "paragraph",
        "text": "Processing workers pick up the content from queues (Kafka/Pub/Sub) and transcode the videos into multiple bitrates (240p, 480p, 720p)"
      },
      {
        "component": "paragraph",
        "text": "After the processing is done the video chunks are now stored into object storage. Each object is of the format {data(blob), metadata, uniqueId}. Unlike local storage, object storage is designed for distributed environments. It uses a flat, non-hierarchical, key value store architecture, in contrast to the tree structures typically used on top of the local block storage."
      },
      {
        "component": "paragraph",
        "text": "A key benefit of object storage is that it avoids hotspots for popular videos by distributing chunks across nodes, which balances the load during access."
      },
      {
        "component": "paragraph",
        "text": "Videos go through a storage lifecycle. When freshly uploaded, they are pushed to CDNs and hot storage for fast access. As they age and demand drops, they move to cold storage, and eventually end up in archive storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Hot Storage → High IOPS (Input/Output per second). Expensive but very fast, typically backed by SSDs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Cold Storage → Higher latency, backed by slower HDDs. Cheaper than hot storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Archive Storage → Retrieval takes minutes to hours, built on magnetic tapes and deep archive storage infra. Cheapest option."
      }
    ]
  }
]
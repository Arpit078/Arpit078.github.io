[
  {
    "id": "25ce9933-5fca-805b-8041-df7e15ca1191",
    "created_time": "2025-08-27T17:38:00.000Z",
    "title": "How do food delivery apps like swiggy, zomato find the nearest restaurants in milliseconds",
    "date": "2025-08-27",
    "description": [
      {
        "component": "paragraph",
        "text": "When you start thinking about it, the problem looks very simple: just store the restaurants‚Äô coordinates, and once the user opens the app, calculate the distance to the restaurants, sort them, and return the results. Easy, right? Yes, until your app grows beyond being a pet project. When the user base reaches millions or even billions, optimized approaches are required."
      },
      {
        "component": "paragraph",
        "text": "One way to handle this is by using geo-enabled databases and spatial indexing."
      },
      {
        "component": "paragraph",
        "text": "For example, insert your restaurants into a geo-enabled database like PostGIS (Postgres) or MongoDB. Then build spatial indexes on the database. Internally, these use R-Trees (rectangle trees), where nearby restaurants are stored within the same rectangles."
      },
      {
        "component": "paragraph",
        "text": "When a user opens the app at a given location (lat, lon), the backend runs a query with a radius (e.g., all restaurants within 5 km). The index filters out relevant restaurants using bounding rectangles with the user location rectangle, and then the exact distance is calculated for the candidates. This is a workable approach when the number of rows is in the millions."
      },
      {
        "component": "paragraph",
        "text": "However, when dealing with billions of rows, geohashing is often used."
      },
      {
        "component": "paragraph",
        "text": "In this setup, the world is divided into grid cells, and each cell is assigned a unique ID (a geohash). Locations that are close to each other have similar geohash prefixes. The algorithm for computing a geohash is straightforward:"
      },
      {
        "component": "numbered_list_item",
        "text": "Take the user‚Äôs longitude and latitude. The longitude range is [-180, 180] and the latitude range is [-90, 90]."
      },
      {
        "component": "numbered_list_item",
        "text": "Create a bit array (size depends on the precision required). Each geohash character requires 5 bits."
      },
      {
        "component": "numbered_list_item",
        "text": "Start with the longitude: if it lies in the upper half of the range, set the bit; otherwise, leave it unset."
      },
      {
        "component": "numbered_list_item",
        "text": "Halve the range (e.g., [0, 180]) and do the same for latitude to set the next bit."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat this process until the array is filled."
      },
      {
        "component": "numbered_list_item",
        "text": "Take 5 bits at a time, convert them to base32, and map them to a character from '0123456789bcdefghjkmnpqrstuvwxyz'."
      },
      {
        "component": "numbered_list_item",
        "text": "Repeat until you obtain the final geohash string."
      },
      {
        "component": "paragraph",
        "text": "The more characters in the geohash, the more precise the location. For example:"
      },
      {
        "component": "bulleted_list_item",
        "text": "5 characters ‚Üí ~4.9 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "6 characters ‚Üí ~1.2 km precision"
      },
      {
        "component": "bulleted_list_item",
        "text": "7 characters ‚Üí ~152 m precision"
      },
      {
        "component": "paragraph",
        "text": "Food delivery apps usually use 6‚Äì7 characters for querying. The query simply looks for restaurants with a geohash equal to the user‚Äôs geohash or its neighbors. This is a significantly faster approach than calculating distances for every restaurant in the database. üòÇ"
      }
    ]
  },
  {
    "id": "25be9933-5fca-8026-8dbd-c96403b39a0c",
    "created_time": "2025-08-26T16:34:00.000Z",
    "title": "Elasticsearch: What is it really, and why is it so fast?",
    "date": "2025-08-26",
    "description": [
      {
        "component": "paragraph",
        "text": "Elasticsearch is a search engine program that sits on top of your database to make your search queries blazingly fast. Where SQL might take minutes Elasticsearch responds within milliseconds."
      },
      {
        "component": "paragraph",
        "text": "It supports full text searches, aggregations, geo queries, autocompletes and a lot more. "
      },
      {
        "component": "paragraph",
        "text": "The architecture involved in setting up Elasticsearch system is as follows : "
      },
      {
        "component": "bulleted_list_item",
        "text": "Data source - The source of Truth : Postgres, MySQL, MongoDB"
      },
      {
        "component": "bulleted_list_item",
        "text": "Ingestion pipeline- To  copy or stream the data into Elasticsearch from the database."
      },
      {
        "component": "bulleted_list_item",
        "text": "Elasticsearch cluster - Multiple nodes with primary and replica shards of indexes(A big Index over the whole database is broken down into multiple smaller shards so that you can query all of the index in one go with parallel searches on these shards pretty clever!)"
      },
      {
        "component": "bulleted_list_item",
        "text": "Search API - search queries are done via REST API (JSON over HTTP)"
      },
      {
        "component": "paragraph",
        "text": "Okay Elasticsearch is great and all but how does it do it?"
      },
      {
        "component": "paragraph",
        "text": "A very rudimentary view of the inner workings can be taken by the following illustration :"
      },
      {
        "component": "paragraph",
        "text": "1Ô∏è‚É£ Data ingestion in the Elasticsearch - data is inserted using POST API"
      },
      {
        "component": "paragraph",
        "text": "POST /book/_doc/1 "
      },
      {
        "component": "paragraph",
        "text": "{‚Äútitle‚Äù : ‚ÄúHarry  Potter‚Äù, ‚Äúauthor‚Äù : ‚ÄúJ. K. Rowling‚Äù, ‚Äúyear‚Äù : 1997}"
      },
      {
        "component": "paragraph",
        "text": "2Ô∏è‚É£ Inverted indexes are created - Think of these indexes as the word indexes found at the end of books. Tokens are mapped to the documents they occurred in."
      },
      {
        "component": "paragraph",
        "text": "‚ÄúHarry‚Äù ‚Üí Doc1"
      },
      {
        "component": "paragraph",
        "text": "‚ÄúPotter‚Äù ‚Üí Doc1"
      },
      {
        "component": "paragraph",
        "text": "‚ÄúHogwarts‚Äù ‚Üí Doc2"
      },
      {
        "component": "paragraph",
        "text": "3Ô∏è‚É£ Scoring and Ranking for search - Search inputs are broken into tokens and matched with the relevant documents using the Inverted Indexes. Docs having the highest occurrence of the token are given higher scores. Docs which contain the rarer word are given higher score(‚ÄùHogwarts‚Äù is rarer than ‚Äúthe‚Äù). The higher the score the higher the rank."
      },
      {
        "component": "paragraph",
        "text": "4Ô∏è‚É£ Indexes are broken into smaller shards and spread across multiple nodes, for faster lookups."
      },
      {
        "component": "paragraph",
        "text": "One more notable property of Elasticsearch is that it stores the fields in a columnar fashion. Which significantly boosts the aggregation speeds. Now to get averages instead of going to multiple rows you are only required to traverse all the entries in the column."
      },
      {
        "component": "paragraph",
        "text": "What it looks like is : "
      },
      {
        "component": "paragraph",
        "text": "Field : year ‚Üí [1997 2005 1996]"
      },
      {
        "component": "paragraph",
        "text": "Field : prices ‚Üí [20 15 25] "
      },
      {
        "component": "paragraph",
        "text": "That sums up, absolute basics one should know about Elasticsearch."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-80d3-9531-cab57ede7228",
    "created_time": "2025-08-25T14:42:00.000Z",
    "title": "Bloom Filters? You've probably read the name somewhere.",
    "date": "2025-08-25",
    "description": [
      {
        "component": "paragraph",
        "text": "Bloom filter is a data structure used to test whether an element is a member of a set or not. It is probabilistic in nature, what that means is that sometimes it may return false positives (i.e., it may return true even if the element does not exist in the set), while it never returns a false negative. If the bloom filter says the element is not a member then it is not a member 100%. It trades a little bit of accuracy for big gains in speed and memory efficiency."
      },
      {
        "component": "paragraph",
        "text": "Bloom filters are very handy in large scale systems like in key value stores, distributed HashMap where filtering out which node might contain the element saves a lot of time."
      },
      {
        "component": "paragraph",
        "text": "The implementation for bloom filters is very easy to be honest, all you need is a bit array and some hash functions. Let‚Äôs take an example :"
      },
      {
        "component": "paragraph",
        "text": "Consider a bit array of size 10 = [0 0 0 0 0 0 0 0 0 0] and 2 hash functions H1 and H2"
      },
      {
        "component": "paragraph",
        "text": "Let‚Äôs insert a key ‚Äúcat‚Äù in our filter :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(‚Äùcat‚Äù) = 2; idx2 =  H2(‚Äùcat‚Äù) = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "Set all the bits at indexes idx1 and idx2 in the bit array."
      },
      {
        "component": "bulleted_list_item",
        "text": "Updated bit array : [0 0 1 0 0 0 0 1 0 0]"
      },
      {
        "component": "paragraph",
        "text": "We‚Äôve inserted the key."
      },
      {
        "component": "paragraph",
        "text": "Now to query a key ‚Äúdog‚Äù follow these steps :"
      },
      {
        "component": "bulleted_list_item",
        "text": "Apply all the hash functions on the key - idx1 = H1(‚Äùdog‚Äù) = 4; idx2 = H2(‚Äùdog‚Äù)  = 7"
      },
      {
        "component": "bulleted_list_item",
        "text": "check all the bits in the bit array at the idx1 and idx2."
      },
      {
        "component": "bulleted_list_item",
        "text": "If all the bits are set the element is in the set else not. Clearly dog is not a member."
      },
      {
        "component": "paragraph",
        "text": "This procedure of querying explains the reason for false positives as there might be collisions, the bits might be set by some other key but match the same pattern of another key not in the set."
      },
      {
        "component": "paragraph",
        "text": "In production the size of the bit arrays is reasonable, for storing 1 Million items the size required is 9.2 Million bits roughly 1.2 MB and 7 hash functions. The setup responds with 1% of false positives."
      },
      {
        "component": "paragraph",
        "text": "When not to use a Bloom filter (important to know)"
      },
      {
        "component": "paragraph",
        "text": "1Ô∏è‚É£ When false positives are not allowed. Like Checking if a password hash exists in a breach database. A false positive could wrongly tell a user their password is compromised."
      },
      {
        "component": "paragraph",
        "text": "2Ô∏è‚É£ When the dataset keeps on changing frequently, because bloom filters do not provide a method for key deletion as two or more keys might have set a common bit. But aren‚Äôt key value stores meant to be constantly changing datasets? Well yes they are! but they have a very clever solution in place which can be discussed in some other article."
      }
    ]
  },
  {
    "id": "25ae9933-5fca-8076-97c5-c03d05e7e1fb",
    "created_time": "2025-08-25T10:13:00.000Z",
    "title": "You heard MongoDB follows ACID properties while other NoSQL DBs don‚Äôt. Do you know How and Why?",
    "date": "2025-08-24",
    "description": [
      {
        "component": "paragraph",
        "text": "TL;DR : NoSQL databases are BASE(Basically Available Soft State Eventually Consistent) by choice not by any limitation to NoSQL data storage format. ACID databases have a very specific use case in scenarios where consistency can‚Äôt be compromised say for Banking applications, Healthcare data storage. MongoDB tries to enter these use cases by being ACID. "
      },
      {
        "component": "paragraph",
        "text": "Long Answer : Unlike SQL databases, which were designed for single-node architectures, NoSQL databases must prioritize availability across multiple nodes. At scale, partition tolerance is a necessity if there‚Äôs a network outage between nodes, the database should still function. To achieve this, many NoSQL systems choose to be AP (Available and Partition-tolerant) as per the CAP theorem, since they cannot be fully consistent and AP at the same time."
      },
      {
        "component": "paragraph",
        "text": "But this still doesn‚Äôt explain why most NoSQL databases aren‚Äôt ACID. The term ACID, made popular by SQL databases, requires that a compliant database ensures:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity: In a transaction either all the operations are successful or none of them are."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency: After a transaction the DB should remain in a valid state, always respecting constraints such as schemas."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation: Every transaction should happen in Isolation, such that no other transaction is affected. This is often implemented using MVCC (Multi-Version Concurrency Control), which I‚Äôll cover later."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability: Once committed, a transaction is permanent and crash-safe, often implemented via Write-Ahead Logging (WAL), also discussed later."
      },
      {
        "component": "paragraph",
        "text": "By definition ACID appears a transaction specific property, no relationship with what it has to do with the database being SQL or NoSQL."
      },
      {
        "component": "paragraph",
        "text": "First thing an ACID database should have atomicity and durability which is often implemented using Write ahead logging(WAL) = a special sequential log file where all the changes are written before they are applied to the actual database. Secondly for Isolation, typically achieved through Multi version concurrency Control(MVCC) Instead of overwriting data mid-transaction, the database maintains multiple versions: one before the transaction and one after it completes. This ensures other operations aren‚Äôt affected by an incomplete transaction. A less common alternative is to enforce isolation using locking mechanisms."
      },
      {
        "component": "paragraph",
        "text": "These performance overheads are the reason most NoSQL databases, which focus on being lightweight and fast, drop full ACID compliance. And that makes sense actually, the majority of NoSQL use cases involve data where immediate consistency isn‚Äôt critical, such as likes, comment counts, or user profiles. In such scenarios, eventual consistency is perfectly acceptable. "
      },
      {
        "component": "paragraph",
        "text": "\nTo support consistency-sensitive use cases such as financial or healthcare data, MongoDB provides full ACID support by making key architectural changes:"
      },
      {
        "component": "bulleted_list_item",
        "text": "Atomicity : MongoDB stores data in BSON format, where each document is a contiguous unit on the disk, making single-document updates atomic. It extends this further with multi-document transactions using Write-Ahead Logging (WAL). While most NoSQL databases (like Cassandra) ensure only document-level atomicity, MongoDB supports atomic operations across multiple documents."
      },
      {
        "component": "bulleted_list_item",
        "text": "Consistency : Document-level consistency can be enforced through schemas. Note: this is different from the ‚Äúconsistency‚Äù defined in the CAP theorem."
      },
      {
        "component": "bulleted_list_item",
        "text": "Isolation : MongoDB uses document-level locking for isolation. During transactions, it provides snapshot isolation, ensuring reads within a transaction see a stable view of the data."
      },
      {
        "component": "bulleted_list_item",
        "text": "Durability : MongoDB uses a Write-Ahead Log (WAL) called WiredTiger, which provides a configurable property called WriteConcern to manage durability requirements, tunable by the user."
      },
      {
        "component": "paragraph",
        "text": "In short, MongoDB captures the benefits of an ACID-compliant database while keeping overhead minimal, whereas other NoSQL databases often prioritize raw speed and flexibility.\n\n"
      }
    ]
  },
  {
    "id": "25ae9933-5fca-808d-bad4-cf03990771c8",
    "created_time": "2025-08-25T10:12:00.000Z",
    "title": "20 million videos are uploaded to YouTube everyday, Figures for short form video content on instagram and tiktok are beyond comprehension. How do all these platforms handle this massive scale?",
    "date": "2025-08-23",
    "description": [
      {
        "component": "paragraph",
        "text": "When a user uploads a video, the media is chunked on the client. For example, a 200 MB file is split into 40 chunks of 5 MB each before the upload starts. Following that all the chunks are uploaded in parallel to increase upload speed. If a chunk upload fails during the process, only that specific chunk is retried, and once successful, it replaces the incomplete chunk in raw storage."
      },
      {
        "component": "paragraph",
        "text": "The first upload is done to the raw storage until further processing pipelines like transcoding are completed. Raw storage is basically a staging area for the media files and act as temporary buckets. "
      },
      {
        "component": "paragraph",
        "text": "Processing workers pick up the content from queues (Kafka/Pub/Sub) and transcode the videos into multiple bitrates (240p, 480p, 720p)"
      },
      {
        "component": "paragraph",
        "text": "After the processing is done the video chunks are now stored into object storage. Each object is of the format {data(blob), metadata, uniqueId}. Unlike local storage, object storage is designed for distributed environments. It uses a flat, non-hierarchical, key value store architecture, in contrast to the tree structures typically used on top of the local block storage."
      },
      {
        "component": "paragraph",
        "text": "A key benefit of object storage is that it avoids hotspots for popular videos by distributing chunks across nodes, which balances the load during access."
      },
      {
        "component": "paragraph",
        "text": "Videos go through a storage lifecycle. When freshly uploaded, they are pushed to CDNs and hot storage for fast access. As they age and demand drops, they move to cold storage, and eventually end up in archive storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Hot Storage ‚Üí High IOPS (Input/Output per second). Expensive but very fast, typically backed by SSDs."
      },
      {
        "component": "bulleted_list_item",
        "text": "Cold Storage ‚Üí Higher latency, backed by slower HDDs. Cheaper than hot storage."
      },
      {
        "component": "bulleted_list_item",
        "text": "Archive Storage ‚Üí Retrieval takes minutes to hours, built on magnetic tapes and deep archive storage infra. Cheapest option."
      }
    ]
  }
]